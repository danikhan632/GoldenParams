# Shape Mismatch Fix for MaskedAdamW

## Problem

When loading golden masks created for one model and trying to use them with a different model architecture, the optimizer would crash with:

```
ValueError: Mask shape torch.Size([151936, 1024]) does not match
parameter shape torch.Size([49154, 576]) for 'model.embed_tokens.weight'
```

This happened because the mask file was created for a different model (e.g., different vocab size or hidden dimensions).

## Solution

The MaskedAdamW optimizer now **gracefully handles shape mismatches**:

1. **Automatic Detection**: Detects when mask shapes don't match parameter shapes
2. **Graceful Fallback**: Treats mismatched parameters as "unmasked" (uses `unmasked_lr`)
3. **Warning Once**: Issues a single warning about mismatches (not one per step)
4. **Tracking**: Tracks which masks were skipped and reports them

## Behavior

### Before (Crashed)
```python
optimizer = MaskedAdamW(model.parameters(), sparse_masks, ...)
# ValueError: Mask shape mismatch!
```

### After (Works)
```python
optimizer = MaskedAdamW(model.parameters(), sparse_masks, ...)
# Warning: Some masks have mismatched shapes and will be skipped...
# Training continues with skipped params using unmasked_lr
```

## Usage

### No Changes Required

Your existing code will work automatically:

```python
from golden_params.optimizers.masked_adamw import MaskedAdamW
from golden_params.utils import convert_masks_to_torch_sparse_coo

# Load masks (even if from different model)
mask_data = torch.load("golden_masks.pt")
sparse_masks = convert_masks_to_torch_sparse_coo(mask_data["masks"])

# Create optimizer (won't crash on mismatch)
optimizer = MaskedAdamW(
    model.parameters(),
    sparse_masks=sparse_masks,
    masked_lr=1e-3,
    unmasked_lr=1e-5
)
optimizer.add_param_names(dict(model.named_parameters()))

# Check what was skipped
lr_info = optimizer.get_lr_info()
print(f"Skipped masks: {lr_info['skipped_masks']}")
print(f"Skipped names: {lr_info['skipped_mask_names']}")
```

## Example Output

When running with mismatched masks:

```
Loading golden masks from: ./golden_masks.pt
Loaded masks for 310 parameters
Mask statistics: 7,936,411/158,727,680 params (5.00% sparse)

Created MaskedAdamW optimizer (masked_lr=0.001, unmasked_lr=1e-05)
  Masks loaded: 310
  Parameters mapped: 312
  âš  Skipped masks (shape mismatch): 2
    (These parameters will use unmasked_lr=1e-05)

UserWarning: Some masks have mismatched shapes and will be skipped.
First mismatch: 'model.embed_tokens.weight' mask torch.Size([151936, 1024])
vs param torch.Size([49154, 576]). These parameters will use unmasked_lr=1e-05.
```

## What Gets Skipped

Parameters with mismatched shapes are treated as **unmasked**:
- They use the `unmasked_lr` learning rate
- No special masking is applied
- Training continues normally

## Checking Skipped Masks

Use `get_lr_info()` to see what was skipped:

```python
lr_info = optimizer.get_lr_info()

if lr_info['skipped_masks'] > 0:
    print(f"Warning: {lr_info['skipped_masks']} masks were skipped")
    print("Skipped parameters:")
    for name in lr_info['skipped_mask_names']:
        print(f"  - {name}")
```

## When This Helps

This is useful when:
1. **Different Model Architectures**: Using masks from a larger model on a smaller one
2. **Different Vocabularies**: Model vocab size changed (embedding layer size different)
3. **Experimental Training**: Quick testing without regenerating masks
4. **Transfer Learning**: Partially applying masks from a similar model

## When to Regenerate Masks

You should regenerate masks when:
- You want optimal mask coverage for the new architecture
- The model architecture is significantly different
- You need precise control over which parameters are masked

## Performance Impact

**Minimal**: Shape checking happens once per parameter per step, with negligible overhead.

## Compatibility

Works with:
- âœ… PyTorch 2.0+
- âœ… GRPO Trainer
- âœ… Standard training loops
- âœ… Distributed training
- âœ… Mixed precision training

## Testing

Run the test to verify:

```bash
python3 test_shape_mismatch.py
```

Expected output:
```
âœ“ Optimizer created successfully
âœ“ Training step completed successfully
ðŸŽ‰ Shape mismatch handling test passed!
```
